import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from pathlib import Path
import os
import glob

class TextFrequencyAnalyzer:
    def __init__(self):
        self.texts = []
        self.word_frequencies = {}
        
    def add_text(self, text, source_name="Texto"):
        """Adiciona um texto √† an√°lise"""
        self.texts.append({
            'text': text,
            'source': source_name
        })
    
    def load_text_file(self, file_path, encoding='utf-8'):
        """Carrega texto de um arquivo"""
        try:
            with open(file_path, 'r', encoding=encoding) as file:
                text = file.read()
                file_name = Path(file_path).stem
                self.add_text(text, file_name)
                print(f"Arquivo '{file_name}' carregado com sucesso!")
        except Exception as e:
            print(f"Erro ao carregar arquivo: {e}")
    
    def analyze_individual_files(self, target_words, case_sensitive=False):
        """
        Analisa cada arquivo individualmente e retorna resultados detalhados
        """
        if not case_sensitive:
            target_words = [word.lower() for word in target_words]
        
        individual_results = {}
        
        for text_data in self.texts:
            text = text_data['text']
            source = text_data['source']
            
            if not case_sensitive:
                processed_text = self.preprocess_text(text)
            else:
                processed_text = text
            
            words = processed_text.split()
            word_count = Counter(words)
            total_words = len(words)
            
            # Resultados para este arquivo
            file_results = {
                'total_palavras': total_words,
                'palavras_encontradas': {},
                'percentuais': {}
            }
            
            for target_word in target_words:
                frequency = word_count.get(target_word, 0)
                percentage = (frequency / total_words * 100) if total_words > 0 else 0
                
                file_results['palavras_encontradas'][target_word] = frequency
                file_results['percentuais'][target_word] = round(percentage, 2)
            
            individual_results[source] = file_results
        
        return individual_results
    
    def create_detailed_report(self, target_words, case_sensitive=False):
        """Cria relat√≥rio detalhado da an√°lise individual"""
        individual_results = self.analyze_individual_files(target_words, case_sensitive)
        
        print("=" * 80)
        print("RELAT√ìRIO DETALHADO - AN√ÅLISE POR ARQUIVO")
        print("=" * 80)
        
        for arquivo, dados in individual_results.items():
            print(f"\nüìÑ ARQUIVO: {arquivo}")
            print(f"   Total de palavras: {dados['total_palavras']}")
            print(f"   Palavras analisadas:")
            
            for palavra in target_words:
                freq = dados['palavras_encontradas'].get(palavra, 0)
                perc = dados['percentuais'].get(palavra, 0)
                print(f"   ‚Ä¢ {palavra}: {freq} ocorr√™ncias ({perc}%)")
        
        return individual_results
    
    def plot_individual_comparison(self, target_words, case_sensitive=False, figsize=(15, 8)):
        """Cria gr√°fico comparando palavras entre diferentes arquivos"""
        individual_results = self.analyze_individual_files(target_words, case_sensitive)
        
        # Preparar dados para o gr√°fico
        data_for_plot = []
        for arquivo, dados in individual_results.items():
            for palavra in target_words:
                freq = dados['palavras_encontradas'].get(palavra, 0)
                data_for_plot.append({
                    'Arquivo': arquivo,
                    'Palavra': palavra,
                    'Frequ√™ncia': freq
                })
        
        df_plot = pd.DataFrame(data_for_plot)
        
        # Criar gr√°fico
        plt.figure(figsize=figsize)
        
        # Gr√°fico de barras agrupadas
        pivot_df = df_plot.pivot(index='Arquivo', columns='Palavra', values='Frequ√™ncia').fillna(0)
        ax = pivot_df.plot(kind='bar', figsize=figsize, width=0.8)
        
        plt.title('Frequ√™ncia de Palavras por Arquivo', fontsize=16, fontweight='bold')
        plt.xlabel('Arquivos', fontsize=12)
        plt.ylabel('Frequ√™ncia', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.legend(title='Palavras', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()
        
        # Gr√°fico de heatmap
        if len(individual_results) > 1:
            plt.figure(figsize=(12, 8))
            sns.heatmap(pivot_df.T, annot=True, cmap='YlOrRd', fmt='g', cbar_kws={'label': 'Frequ√™ncia'})
            plt.title('Mapa de Calor - Distribui√ß√£o de Palavras por Arquivo', fontsize=14, fontweight='bold')
            plt.xlabel('Arquivos', fontsize=12)
            plt.ylabel('Palavras', fontsize=12)
            plt.tight_layout()
            plt.show()
    
    def export_results_to_csv(self, target_words, filename="resultados_mineracao.csv", case_sensitive=False):
        """Exporta resultados para CSV"""
        individual_results = self.analyze_individual_files(target_words, case_sensitive)
        
        # Preparar dados para CSV
        data_for_csv = []
        for arquivo, dados in individual_results.items():
            row = {'Arquivo': arquivo, 'Total_Palavras': dados['total_palavras']}
            
            for palavra in target_words:
                freq = dados['palavras_encontradas'].get(palavra, 0)
                perc = dados['percentuais'].get(palavra, 0)
                row[f'{palavra}_Frequencia'] = freq
                row[f'{palavra}_Percentual'] = perc
            
            data_for_csv.append(row)
        
        df_csv = pd.DataFrame(data_for_csv)
        df_csv.to_csv(filename, index=False, encoding='utf-8')
        print(f"‚úì Resultados exportados para: {filename}")
        return df_csv
        """
        Carrega todos os arquivos de texto de uma pasta
        
        Args:
            folder_path: caminho para a pasta
            file_extensions: lista de extens√µes (ex: ['.txt', '.md']) ou None para todas
            encoding: codifica√ß√£o dos arquivos
        """
        if file_extensions is None:
            file_extensions = ['.txt', '.md', '.doc', '.docx']
        
        folder_path = Path(folder_path)
        
        if not folder_path.exists():
            print(f"Erro: Pasta n√£o encontrada: {folder_path}")
            return
        
        files_loaded = 0
        
        # Procurar por todos os tipos de arquivo
        for ext in file_extensions:
            pattern = folder_path / f"*{ext}"
            files = glob.glob(str(pattern))
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='ignore') as file:
                        text = file.read()
                        file_name = Path(file_path).stem
                        self.add_text(text, file_name)
                        files_loaded += 1
                        print(f"‚úì Arquivo carregado: {file_name}")
                except Exception as e:
                    print(f"‚úó Erro ao carregar {file_path}: {e}")
        
        if files_loaded == 0:
            print(f"Nenhum arquivo encontrado em: {folder_path}")
            print(f"Extens√µes procuradas: {file_extensions}")
        else:
            print(f"\nTotal de arquivos carregados: {files_loaded}")
    
    def load_all_files_from_folder(self, folder_path, encoding='utf-8'):
        """
        Carrega TODOS os arquivos de uma pasta, independente da extens√£o
        """
        folder_path = Path(folder_path)
        
        if not folder_path.exists():
            print(f"Erro: Pasta n√£o encontrada: {folder_path}")
            return
        
        files_loaded = 0
        
        # Pegar todos os arquivos da pasta
        all_files = [f for f in folder_path.iterdir() if f.is_file()]
        
        for file_path in all_files:
            try:
                # Tentar ler como texto
                with open(file_path, 'r', encoding=encoding, errors='ignore') as file:
                    text = file.read()
                    if text.strip():  # S√≥ adiciona se n√£o estiver vazio
                        file_name = file_path.stem
                        self.add_text(text, file_name)
                        files_loaded += 1
                        print(f"‚úì Arquivo carregado: {file_name} ({file_path.suffix})")
            except Exception as e:
                print(f"‚úó Erro ao carregar {file_path.name}: {e}")
        
        if files_loaded == 0:
            print(f"Nenhum arquivo de texto encontrado em: {folder_path}")
        else:
            print(f"\nTotal de arquivos carregados: {files_loaded}")
    
    
    def preprocess_text(self, text):
        """Preprocessa o texto (remove pontua√ß√£o, converte para min√∫sculas)"""
        # Remove pontua√ß√£o e caracteres especiais, mant√©m apenas letras e espa√ßos
        text = re.sub(r'[^\w\s]', ' ', text)
        # Converte para min√∫sculas
        text = text.lower()
        # Remove espa√ßos extras
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def analyze_frequency(self, target_words, case_sensitive=False):
        """
        Analisa a frequ√™ncia das palavras especificadas
        
        Args:
            target_words: lista de palavras para analisar
            case_sensitive: se True, considera mai√∫sculas/min√∫sculas
        """
        if not case_sensitive:
            target_words = [word.lower() for word in target_words]
        
        self.word_frequencies = {word: [] for word in target_words}
        
        for text_data in self.texts:
            text = text_data['text']
            source = text_data['source']
            
            if not case_sensitive:
                processed_text = self.preprocess_text(text)
            else:
                processed_text = text
            
            words = processed_text.split()
            word_count = Counter(words)
            
            # Conta a frequ√™ncia de cada palavra alvo
            for target_word in target_words:
                frequency = word_count.get(target_word, 0)
                self.word_frequencies[target_word].append({
                    'source': source,
                    'frequency': frequency
                })
        
        return self.word_frequencies
    
    def create_frequency_dataframe(self):
        """Cria um DataFrame com os resultados da an√°lise"""
        data = []
        for word, freq_list in self.word_frequencies.items():
            for freq_data in freq_list:
                data.append({
                    'palavra': word,
                    'fonte': freq_data['source'],
                    'frequencia': freq_data['frequency']
                })
        
        return pd.DataFrame(data)
    
    def plot_frequency_bar(self, figsize=(12, 6), title="Frequ√™ncia de Palavras"):
        """Cria gr√°fico de barras da frequ√™ncia das palavras"""
        df = self.create_frequency_dataframe()
        
        plt.figure(figsize=figsize)
        
        if len(self.texts) == 1:
            # Se h√° apenas um texto, mostra frequ√™ncia simples
            word_totals = df.groupby('palavra')['frequencia'].sum().sort_values(ascending=False)
            plt.bar(word_totals.index, word_totals.values)
            plt.title(title)
            plt.xlabel('Palavras')
            plt.ylabel('Frequ√™ncia')
            plt.xticks(rotation=45)
        else:
            # Se h√° m√∫ltiplos textos, mostra compara√ß√£o
            pivot_df = df.pivot(index='palavra', columns='fonte', values='frequencia').fillna(0)
            pivot_df.plot(kind='bar', figsize=figsize)
            plt.title(title)
            plt.xlabel('Palavras')
            plt.ylabel('Frequ√™ncia')
            plt.xticks(rotation=45)
            plt.legend(title='Fonte', bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout()
        plt.show()
    
    def plot_frequency_heatmap(self, figsize=(10, 6)):
        """Cria um heatmap da frequ√™ncia das palavras (√∫til para m√∫ltiplos textos)"""
        if len(self.texts) <= 1:
            print("Heatmap requer m√∫ltiplos textos para compara√ß√£o.")
            return
        
        df = self.create_frequency_dataframe()
        pivot_df = df.pivot(index='palavra', columns='fonte', values='frequencia').fillna(0)
        
        plt.figure(figsize=figsize)
        sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', fmt='g')
        plt.title('Mapa de Calor - Frequ√™ncia de Palavras por Fonte')
        plt.tight_layout()
        plt.show()
    
    def get_summary_stats(self):
        """Retorna estat√≠sticas resumidas da an√°lise"""
        df = self.create_frequency_dataframe()
        summary = df.groupby('palavra').agg({
            'frequencia': ['sum', 'mean', 'std', 'max', 'min']
        }).round(2)
        
        summary.columns = ['Total', 'M√©dia', 'Desvio Padr√£o', 'M√°ximo', 'M√≠nimo']
        return summary

# Exemplo de uso
if __name__ == "__main__":
    # Criar inst√¢ncia do analisador
    analyzer = TextFrequencyAnalyzer()
    
    # CONFIGURA√á√ÉO PARA SUA PASTA DE ARTIGOS
    pasta_artigos = r"C:\Users\PICHAU\Downloads\artigos_selecionados\artigos\ciencia_de_dados"
    
    print("=== CARREGANDO ARQUIVOS DA PASTA ===")
    
    # OP√á√ÉO 1: Carregar apenas arquivos espec√≠ficos (.txt, .md, etc.)
    # analyzer.load_folder(pasta_artigos, file_extensions=['.txt', '.md', '.doc'])
    
    # OP√á√ÉO 2: Carregar TODOS os arquivos da pasta (recomendado)
    analyzer.load_all_files_from_folder(pasta_artigos)
    
    # Verificar se arquivos foram carregados
    if not analyzer.texts:
        print("‚ùå Nenhum arquivo foi carregado!")
        print("Verifique se:")
        print("1. O caminho est√° correto")
        print("2. Existem arquivos na pasta")
        print("3. Os arquivos cont√™m texto")
        
        # Tentar caminhos alternativos
        caminhos_alternativos = [
            r"C:\Users\PICHAU\Downloads\artigos_selecionados\artigos",
            r"C:\Users\PICHAU\Downloads\artigos_selecionados",
            r"C:\Users\PICHAU\Downloads"
        ]
        
        print("\nTentando caminhos alternativos...")
        for caminho in caminhos_alternativos:
            if os.path.exists(caminho):
                print(f"‚úì Pasta encontrada: {caminho}")
                arquivos = list(Path(caminho).iterdir())
                print(f"  Arquivos na pasta: {len([f for f in arquivos if f.is_file()])}")
                break
            else:
                print(f"‚úó Pasta n√£o encontrada: {caminho}")
    
    else:
        print(f"‚úÖ {len(analyzer.texts)} arquivos carregados com sucesso!")
        
        # SUAS PALAVRAS-CHAVE PARA AN√ÅLISE
        # *** MODIFIQUE AQUI COM AS PALAVRAS QUE VOC√ä QUER ANALISAR ***
        palavras_alvo = [
            "architecture", "security","privacy"
        ]
        
        print(f"\n=== ANALISANDO PALAVRAS: {palavras_alvo} ===")
        
        # AN√ÅLISE COMPLETA
        print("\n1. RELAT√ìRIO DETALHADO POR ARQUIVO:")
        resultados_individuais = analyzer.create_detailed_report(palavras_alvo)
        
        print("\n2. AN√ÅLISE GERAL:")
        analyzer.analyze_frequency(palavras_alvo)
        df_results = analyzer.create_frequency_dataframe()
        print(df_results.head(10))
        
        print("\n3. ESTAT√çSTICAS RESUMIDAS:")
        print(analyzer.get_summary_stats())
        
        # VISUALIZA√á√ïES
        print("\n4. GERANDO GR√ÅFICOS...")
        
        # Gr√°fico comparativo entre arquivos
        analyzer.plot_individual_comparison(palavras_alvo)
        
        # Gr√°fico geral
        analyzer.plot_frequency_bar(title="Frequ√™ncia Total das Palavras-Chave")
        
        # EXPORTAR RESULTADOS
        print("\n5. EXPORTANDO RESULTADOS...")
        df_exportado = analyzer.export_results_to_csv(
            palavras_alvo, 
            filename="analise_artigos_ciencia_dados.csv"
        )
        
        print("\n=== RESUMO FINAL ===")
        print(f"üìÅ Arquivos analisados: {len(analyzer.texts)}")
        print(f"üîç Palavras pesquisadas: {len(palavras_alvo)}")
        print(f"üìä Resultados exportados para: analise_artigos_ciencia_dados.csv")
        
        # Mostrar os 5 arquivos com mais ocorr√™ncias
        print("\nüèÜ TOP 5 ARQUIVOS COM MAIS PALAVRAS-CHAVE:")
        totais_por_arquivo = {}
        for arquivo, dados in resultados_individuais.items():
            total = sum(dados['palavras_encontradas'].values())
            totais_por_arquivo[arquivo] = total
        
        top_5 = sorted(totais_por_arquivo.items(), key=lambda x: x[1], reverse=True)[:5]
        for i, (arquivo, total) in enumerate(top_5, 1):
            print(f"{i}. {arquivo}: {total} ocorr√™ncias")
    
    print("\n=== INSTRU√á√ïES PARA PERSONALIZAR ===")
    print("1. Modifique a vari√°vel 'palavras_alvo' com suas palavras")
    print("2. Ajuste o caminho 'pasta_artigos' se necess√°rio")
    print("3. Execute o c√≥digo")
    print("4. Os resultados ser√£o salvos em CSV automaticamente")
    
    # Fun√ß√£o para listar arquivos da pasta (diagn√≥stico)
    def diagnosticar_pasta(caminho):
        """Fun√ß√£o para diagnosticar problemas na pasta"""
        print(f"\n=== DIAGN√ìSTICO DA PASTA ===")
        print(f"Caminho: {caminho}")
        
        if not os.path.exists(caminho):
            print("‚ùå Pasta n√£o existe!")
            return
        
        pasta = Path(caminho)
        arquivos = list(pasta.iterdir())
        
        print(f"Total de itens: {len(arquivos)}")
        print(f"Arquivos: {len([f for f in arquivos if f.is_file()])}")
        print(f"Pastas: {len([f for f in arquivos if f.is_dir()])}")
        
        print("\nPrimeiros 10 arquivos encontrados:")
        arquivos_apenas = [f for f in arquivos if f.is_file()][:10]
        for arquivo in arquivos_apenas:
            print(f"  - {arquivo.name} ({arquivo.suffix})")
    
    # Descomente para diagnosticar sua pasta:
    # diagnosticar_pasta(pasta_artigos)